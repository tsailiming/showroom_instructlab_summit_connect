= Advanced InstructLab Workshop: Training a custom LLM for the Parasol Insurance Company

:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

== Introduction to the Lab

Thanks for taking the time to learn about and use InstructLab. During this hands-on exercise, you will learn what InstructLab is and how you can use it to provide value to your company. InstructLab is a core component of https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[RHEL AI], and can be used to contribute to and improve a Large Language Model (LLM).

RHEL AI consists of several core components:

. https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/image-mode[RHEL Image Mode]
. The https://www.ibm.com/granite[Granite] Large Language Model(s)
. https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] for model alignment
. Support and indemnification from Red Hat

https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] is a fully open-source project from Red Hat, and the MIT-IBM Watson AI Lab, that introduces https://arxiv.org/abs/2403.01081[Large-scale Alignment for chatBots] (LAB). The project's innovation helps during the instruction-tuning phase of LLM training. However, to fully understand the benefit of this project, you need to be familiar with some basic concepts of what an LLM is and the difficulty and cost associated with training a model.

[#llms]
=== What is a Large Language Model?

A large language model (LLM) is a type of artificial intelligence (AI) model that uses deep learning techniques to understand and generate human-like text based on input data. These models are designed to analyze vast amounts of text data and learn patterns, relationships, and structures within the data. They can be used for various natural language processing (NLP) tasks, such as:

* *Text classification*: Categorizing text based on its content, such as spam detection or sentiment analysis.
* *Text summarization*: Generating concise summaries of longer texts, such as news articles or research papers.
* *Machine translation*: Translating text from one language to another, such as English to French or German to Chinese.
* *Question answering*: Answering questions based on a given context or set of documents.
* *Text generation*: Creating new text that is coherent, contextually relevant, and grammatically correct, such as writing articles, stories, or even poetry.

Large language models typically have many parameters (millions to billions) that allow them to capture complex linguistic patterns and relationships in the data. They are trained on large datasets, such as books, articles, and websites, using techniques like unsupervised pre-training and supervised fine-tuning. Some popular large language models include GPT-4, Llama, and Mistral.

In summary, a large language model (LLM) is an artificial intelligence model that uses deep learning techniques to understand and generate human-like text based on input data. They are designed to analyze vast amounts of text data and learn patterns, relationships, and structures within the data, and can be used for various natural language processing tasks.

NOTE: To give you an idea of what an LLM can accomplish, the entire previous section was generated with a simple question against the foundational model you are using in this workshop.

[#how_trained]
=== How are Large Language Models trained?

Large language models (LLMs) are typically trained using deep learning techniques and large datasets. The training process involves several steps:

. *Data Collection*: A vast amount of text data is collected from various sources, such as books, articles, websites, and databases. The data may include different languages, domains, and styles to ensure the model can generalize well.
. *Pre-processing*: The raw text data is pre-processed to remove noise, inconsistencies, and irrelevant information. This may include tokenization, lowercasing, stemming, lemmatization, and encoding.
. *Tokenization*: The pre-processed text data is converted into tokens (words or subwords) that can be used as input and output to the model. Some models use byte-pair encoding (BPE) or subword segmentation to create tokens that can handle out-of-vocabulary words and maintain contextual information.
. *Pre-training*: The model is trained in an unsupervised or self-supervised manner to learn patterns and structures in the data.
. *Model Alignment*: (instruction tuning and preference tuning): The process of encoding human values and goals into large language models to make them as helpful, safe, and as reliable as possible. This step is not as compute intensive as some of the other steps.

[#instructlab]
=== How does this relate to InstructLab?

InstructLab leverages a taxonomy-guided synthetic data generation process and a multi-phase tuning framework. This allows InstructLab to significantly reduce reliance on expensive human annotations, making contributing to a large language model easy and accessible. This means that InstructLab can use the LLM to generate data, which is then used to further train the LLM. It also means that the alignment phase becomes most users' starting point for contributing their knowledge.  Prior to the LAB technique, users typically had no direct involvement in training an LLM. I know this may sound complicated, but hang in there. You will see how easy this is to use.

As you work with InstructLab, you will see the terms Skills and Knowledge. What is the difference between Skills and Knowledge? A simple analogy is to think of a skill as teaching someone how to fish. Knowledge, on the other hand, is knowing that the best place to catch a Bass is when the sun is setting while casting your line near the trunk of a tree along the bank.

[#getting_started]
== Getting started with InstructLab

[#installation]
=== Install the ilab command line interface

We created a CLI tool called https://github.com/instructlab/instructlab[ilab] that implements a local LLM developer experience and workflow. The ilab CLI is written in Python and works on the following architectures:

. Apple M1/M2/M3 Mac
. Linux systems
. Windows 11 within a WSL environment

During this lab, we will be using the ilab command line tools on a Red Hat Enterprise Linux system. We anticipate support for more operating systems in the future. The system requirements to use the command line tool are as follows:

. C++ compiler
. Python 3.10 or Python 3.11
. Approximately 60GB disk space (entire process)
.. Disk space requirements are dependent on several factors. Keep in mind that we will be generating data to feed to the model while also having the model locally on our system. For example, the model we are working with during this workshop is roughly 5gb in size.

[#configuration]
=== Configuring ilab

The first thing we need to do is to source a Python virtual environment that will allow us to interact with the InstructLab command line tools. While you have two terminal windows available on your right, let's start with the *upper* window.

. Navigate to the preset InstructLab folder and activate the python virtual environment by running the following commands:
+

[source,console,role=execute,subs=attributes+]
----
cd ~/instructlab
source venv/bin/activate
----
+
.Your prompt should now look like this

[source,console]
----
(venv) [instruct@bastion instructlab]$
----
+

[start=2]
. InstructLab is already pre-installed. From your venv environment, verify ilab is installed correctly by running the ilab command.
+

[source,console,role=execute,subs=attributes+]
----
ilab
----
+

Assuming that everything has been installed correctly, you should see the following output:
+

[subs:quotes]
----
Usage: ilab [OPTIONS] COMMAND [ARGS]...


  CLI for interacting with InstructLab.


  If this is your first time running ilab, it's best to start with `ilab config init`
  to create the environment.


Options:
  --config PATH  Path to a configuration file.  [default: /home/instruct/.config/instructlab/config.yaml]
  -v, --verbose  Enable debug logging (repeat for even more verbosity)
  --version      Show the version and exit.
  --help         Show this message and exit.

Commands:
  config    Command Group for Interacting with...
  data      Command Group for Interacting with...
  model     Command Group for Interacting with...
  system    Command group for all system-related...
  taxonomy  Command Group for Interacting with...

Aliases:
  chat      model chat
  generate  data generate
  serve     model serve
  train     model train
----


*Congratulations!* You now have everything installed and are ready to dive into the world of LLM alignment!

[#initialize]
=== Initialize ilab

Now that we know that the command-line interface `ilab` is working correctly, the next thing we need to do is initialize the local environment so that we can begin working with the model. This is accomplished by issuing a simple init command. Initialize `ilab` by running the following command:

[source,console,role=execute,subs=attributes+]
----
ilab config init
----

You should see the following output (press kbd:[ENTER] for defaults):

[source,console,subs=quotes]
----
Welcome to InstructLab CLI. This guide will help you to setup your environment.
Please provide the following values to initiate the environment [press Enter for defaults]:
Path to taxonomy repo [/home/instruct/.local/share/instructlab/taxonomy]:
----

NOTE: You may hit kbd:[ENTER] for all default settings.

[source,console,subs=quotes]
----
Path to your model [/home/instruct/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:
Generating `/home/instruct/.config/instructlab/config.yaml`...
Detecting Hardware...
We chose Nvidia 1x L4 as your designated training profile. This is for systems with 24 GB of vRAM.
This profile is the best approximation for your system based off of the amount of vRAM. We modified it to match the number of GPUs you have.
Is this profile correct? [Y/n]: Y
----

Type `Y` as shown above or press kbd:[ENTER] to accept the training profile configuration. **For this lab**, we are using a single NVIDIA L4 GPU as described in the above output.

[source,console,subs=quotes]
----
Initialization completed successfully, you're ready to start using `ilab`. Enjoy!
----

* Several things happen during the initialization phase: A default taxonomy is located on the local file system, and a configuration file (config.yaml) is created in the 'home/instruct/.config/instructlab/' directory.
* The config.yaml file contains defaults we will use during this workshop. After this workshop, when you begin playing around with InstructLab, it is important to understand the contents of the configuration file so that you can tune the parameters to your liking.

[#download]
=== Download the model

With the InstructLab environment configured, you will now download two different quantized (compressed and optimized) models to your local directory. Granite will be used as a model server for API requests, and Merlinite will help create synthetic data to train a new model.

NOTE: We are using quantized models because we are only leveraging a single GPU for this lab. For better performance or production use cases, you would use unquantized models.

Run the `ilab model download` commands as shown below:

First, let's download Granite:

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
----

One more time, let's pull down Merlinite:

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository instructlab/merlinite-7b-lab-GGUF --filename=merlinite-7b-lab-Q4_K_M.gguf
----

The `ilab model download` commands download models from the HuggingFace Instructlab organization that we will use for this workshop. The output should look like the following:

[source,console,subs=quotes]
----
Downloading model from Hugging Face: instructlab/granite-7b-lab-GGUF@main to /home/instruct/.cache/instructlab/models...
Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
INFO 2024-09-10 16:51:32,740 huggingface_hub.file_download:1908: Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
granite-7b-lab-Q4_K_M.gguf: 100%|█| 4.08G/4.08G [00:19<00:00, 207
Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
INFO 2024-09-10 16:51:52,562 huggingface_hub.file_download:1924: Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

Now the models are downloaded, we can serve and chat with the Granite model. Serving the model simply means we are going to run a server that will allow other programs to interact with the data similar to making an API call.

[#serve]
=== Serving the model

Let's serve the model by running the following command:

[source,console,role=execute,subs=attributes+]
----
ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

As you can see, the serve command can take an optional `-–model-path` argument. In this case, we want to serve the Granite model. If no model path is provided, the default value from the config.yaml file will be used.

Once the model is served and ready, you’ll see the following output:

[source,console,subs=quotes]
----
INFO 2024-09-10 18:12:09,459 instructlab.model.serve:145: Using model '/home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-09-10 18:12:09,459 instructlab.model.serve:149: Serving model '/home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf' with llama-cpp
INFO 2024-09-10 18:12:16,023 instructlab.model.backends.llama_cpp:250: Replacing chat template:
 {% for message in messages %}
{% if message['role'] == 'user' %}
{{ '<|user|>
' + message['content'] }}
{% elif message['role'] == 'system' %}
{{ '<|system|>
' + message['content'] }}
{% elif message['role'] == 'assistant' %}
{{ '<|assistant|>
' + message['content'] + eos_token }}
{% endif %}
{% if loop.last and add_generation_prompt %}
{{ '<|assistant|>' }}
{% endif %}
{% endfor %}
INFO 2024-09-10 18:12:16,026 instructlab.model.backends.llama_cpp:193: Starting server process, press CTRL+C to shutdown server...
INFO 2024-09-10 18:12:16,026 instructlab.model.backends.llama_cpp:194: After application startup complete see http://127.0.0.1:8000/docs for API.

----

*WOOHOO!* You just served the model for the first time and are ready to test out your work so far by interacting with the LLM. We are going to accomplish this by chatting with the model.

[#chat]
=== Chat with the model

Because you’re serving the model in one terminal window, you will have to use a separate terminal window and re-activate your Python virtual environment to run the `ilab chat` command and communicate with the model you are serving.

. In the *bottom* terminal window, issue the following commands:

[source,console,role=execute,subs=attributes+]
----
cd ~/instructlab
source venv/bin/activate
----

.Your prompt should now look like this
[source,console]
----
(venv) [instruct@bastion instructlab]$
----

[start=2]
. Now that the environment is sourced, you can begin a chat session with the ilab chat command:

[source,console,role=execute,subs=attributes+]
----
ilab model chat -m /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

You should see a chat prompt like the example below.

[source,console]
----
╭───────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ GRANITE-7B-LAB-Q4_K_M.GGUF (type /h for help)
╰───────────────────────────────────────────────────────────────────────────╯
>>>
----

[start=3]
. At this point, you can interact with the model by asking it a question. Example:
What is OpenShift in 20 words or less?

[source,console,role=execute,subs=attributes+]
----
What is OpenShift in 20 words or less?
----

Wait, wut? That was AWESOME!!!!! You now have your own local LLM running on this machine. That was pretty easy, huh?

[#integrating_instructlab]
== Integrating AI into an Insurance Application

The previous section showed you the basics of how to interact with InstructLab. Now let's take things a step further by using InstructLab with an example application. We will use InstructLab to leverage the Granite LLM, add additional data in the form of knowledge and/or skills, train the model with new knowledge and enable it to answer questions effectively. This is done in the context of Parasol, a fictional company that processes insurance claims.

Parasol has a chatbot application infused with AI (the Granite model) to provide repair suggestions for claims submitted. This would allow Parasol to expedite processing of various claims on hold. But at the moment, the chatbot does not provide effective repair suggestions. Using historical claims data that contain different repairs performed under different conditions, we show how users can add this knowledge to the Granite model, train it on the additional knowledge and improve its recommendations.

[#using_parasol_application]
=== Using the Parasol Application

Let's start by taking a look at the current experience a claims agent has when interacting with the chatbot.

. While you may currently be in the *Terminals* view, switch to *Parasol* (in the top bar above the upper terminal window) to see the Parasol company's claims application in your browser.

image::parasol-view.png[]

As a claims agent, you can navigate and view the existing claims by clicking on the claim number on the screen.

[start=2]
. For this lab we will be investigating *CLM195501* which is a claim that has been filed by Marty McFly, let's click on this claim now.

image::parasol-claim.png[]

You can read the details of the claim on this page.

[start=3]
. Once you read the claim, click on the chatbot using the small blue icon in the bottom right of the page.

image::parasol-chat.webp[width=350]

IMPORTANT: This chatbot is backed by the Granite model you served earlier, so if you killed that running process you will need to restart it in your terminal by running the following: `ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf`

Let's imagine as a claims agent you'd like to know how much it might cost to repair a flux capacitor on Marty's DeLorean.

[start=4]
. Ask the chatbot the following question:

[source,console,role=execute,subs=attributes+]
----
How much does it cost to repair a flux capacitor?
----


You should see something similar to the following. Note that LLMs by nature are non-deterministic. This means that even with the same prompt input, the model will produce varying responses. So, your results may vary slightly.

image::parasol-chat-response.webp[width=350]

What we've already started to do is provide contextual information about the claim in each conversation with the LLM using Prompt Engineering. But unfortunately, the chatbot doesn't know how much it costs to repair a flux capacitor, nor will it have any domain-specific knowledge for our organization.

With InstructLab and RHEL AI, we can change that by teaching the model!


[#taxonomy]
=== Understanding the Taxonomy

InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models (LLMs.) The "lab" in InstructLab stands for **L**arge-scale **A**lignment for Chat **B**ots.

The LAB method is driven by taxonomies, which are largely created manually and with care.

InstructLab crowdsources the process of tuning and improving models by collecting two types of data: **knowledge** and **skills**, in the new InstructLab open source community. These submissions are collected in a taxonomy of YAML files to be used in the synthetic data generation process. To help you understand the directory structure of a taxonomy, please refer to the following image.

image::taxonomy.png[]

We are now going to leverage the taxonomy model to teach the model knowledge about a specific vehicle we cover and its details, from our organization's collection of public (and private) internal data.


Navigate back to the *Terminals* view. In the terminal window where you are running chat, enter `exit` to quit the chat session.

. Navigate to the taxonomy directory.

[source,console,role=execute,subs=attributes+]
----
cd /home/instruct/.local/share/instructlab
tree taxonomy | head -n 10
----

.You should see the taxonomy directory listed as shown below:
[source,texinfo]
----
taxonomy
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── geography
│   ├── grounded
│   │   ├── arts
│   │   ├── engineering
│   │   ├── geography
----

Now, we need to create a directory where we can place our files.

[start=2]
. Create a directory to add new knowledge, demonstrating how to properly use the taxonomy structure to add knowledge with InstructLab.

[source,console,role=execute,subs=attributes+]
----
mkdir -p /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims
----

[start=3]
. Add new capabilities to our model through new knowledge.

The way the taxonomy approach works is that we provide a file, named `qna.yaml`, that contains a sample data set of questions and answers. This data set will be used in the process of creating many more synthetic data examples, enough to fully influence the model's output. The important thing to understand about the `qna.yaml` file is that it must follow a specific schema for InstructLab to use it to synthetically generate more examples.

The `qna.yaml` file is placed in a folder within the `knowledge` subdirectory of the taxonomy directory. It is placed in a folder with an appropriate name that is aligned with the data topic, as you will see in the below command.

[start=4]
. Instead of having to type a bunch of information in by hand, simply run the following command to copy this example https://raw.githubusercontent.com/rhai-code/backToTheFuture/main/qna.yaml[`qna.yaml`] file to your taxonomy directory:

[source,console,role=execute,subs=attributes+]
----
cp -av ~/files/backToTheFuture/qna.yaml /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/
----

[start=5]
. You can then verify the file was correctly copied by issuing the following command which will display the first 10 lines of the file:

[source,console,role=execute,subs=attributes+]
----
head /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml
----

During this workshop, we don’t expect you to type all of this information in by hand - we are including the content here for your reference.

It's a YAML file that consists of a list of Q&A examples that will be used by the trainer model to teach the student model. There is also a source document which is a link to a specific commit of a text file in git, where https://github.com/rhai-code/backToTheFuture/blob/main/data.md[we've included] that a flux capacitor costs an affordable $10,000,000.

To help you understand the qna file format, we have included an excerpt of the file below. Feel free to view the entire file on the system with the following command:

[source,console,role=execute,subs=attributes+]
----
cat /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml
----

[source,yaml]
----
version: 3
domain: time_travel
created_by: Marty McFly
seed_examples:
  - context: |
      The DeLorean DMC-12 is a sports car manufactured by John DeLorean's DeLorean Motor Company
      for the American market from 1981 to 1983. The car features gull-wing doors and a stainless-steel body.
      It gained fame for its appearance as the time machine in the "Back to the Future" film trilogy.
    questions_and_answers:
      - question: |
          When was the DeLorean manufactured?
        answer: |
          The DeLorean was manufactured from 1981 to 1983.
      - question: |
          Who manufactured the DeLorean DMC-12?
        answer: |
          The DeLorean Motor Company manufactured the DeLorean DMC-12.
      - question: |
          What type of doors does the DeLorean DMC-12 have?
        answer: |
          Gull-wing doors.
document_outline: |
  Details and repair costs on a DeLorean DMC-12 car.
document:
  repo: https://github.com/gshipley/backToTheFuture.git
  commit: 8bd9220c616afe24b9673d94ec1adce85320809c
  patterns:<6>
    - data.md
----

. `**version**`: The version of the qna.yaml file, this is the format of the file used for SDG (SDG = Synthetic Data Generation). The value must be the number 3.
. `**created_by**`: Your GitHub username.
. `**domain**`: Specify the category of the knowledge.
. `**seed_examples**`: A collection of key/value entries.
.. `**context**`: A chunk of information from the knowledge document. Each qna.yaml needs five context blocks and has a maximum word count of 500 words.
.. `**questions_and_answers**`: The parameter that holds your questions and answers
... `**question**`: Specify a question for the model. Each qna.yaml file needs at least three question and answer pairs per context chunk with a maximum word count of 250 words.
... `**answer**`: Specify the desired answer from the model. Each qna.yaml file needs at least three question and answer pairs per context chunk with a maximum word count of 250 words.
. `**document_outline**`: Describe an overview of the document you're submitting.
. `**document**`: The source of your knowledge contribution.
.. `**repo**`: The URL to your repository that holds your knowledge markdown files.
.. `**commit**`: The SHA of the commit in your repository with your knowledge markdown files.
.. `**patterns**`: A list of glob patterns specifying the markdown files in your repository. Any glob pattern that starts with *, such as *.md, must be quoted due to YAML rules. For example, *.md.

Now, it's time to verify that the seed data is curated properly.

[start=6]
. Validate your taxonomy

InstructLab allows you to validate your taxonomy files before generating additional data. You can accomplish this by using the `ilab taxonomy diff` command as shown below:

NOTE: Make sure you are still in the virtual environment indicated by the (venv) on the command line. If not, source the `venv/bin/activate` file again.

[source,console,role=execute,subs=attributes+]
----
ilab taxonomy diff
----

.You should see the following output:
[source,console]
----
knowledge/parasol/claims/qna.yaml
Taxonomy in /home/instruct/.local/share/instructlab/taxonomy is valid :)
----

[#sdg]
=== Generating Synthetic Data

Okay, so far so good. Now, let’s move on to the AWESOME part. We are going to use our taxonomy, which contains our `qna.yaml` file, to have the LLM automatically generate more examples. The generate step can often take a while and is dependent on your hardware and the amount of synthetic data that you want to generate.

InstructLab will generate X number of additional questions and answers based on the samples provided. To give you an idea, it takes 7 minutes when running the default full synthetic data generation pipeline at a scale factor of 30. This can take around 15 minutes using Apple Silicon and depends on many factors. You could customize the scale factor or run a simple pipeline to take less time or if you have lesser hardware, but it is not recommended as it will not generate the optimal output.

However, for the purpose of this workshop we will only generate a small amount of additional samples to give you a sense of how it works.

NOTE: If needed, stop serving the Granite model by typing kbd:[CTRL+C] in the terminal within which it is running.

We will now run the command (in the second, **bottom** Terminal) to generate the synthetic data. The merlinite model will serve as the **teacher** model:

[source,console,role=execute,subs=attributes+]
----
ilab data generate --model /home/instruct/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf --sdg-scale-factor 5 --pipeline simple --gpus 1
----

After running this command, the magic begins!

NOTE: You will see an `AssertionError` thrown before the SDG process begins. This does not impact the process so please continue without worry.

InstructLab is now synthetically generating data based on the seed data you provided in the `qna.yaml` file.

You will see output on your screen indicating the data is being generated as shown below:

[source,console]
----
INFO 2024-10-21 02:01:23,450 instructlab.sdg.llmblock:51: LLM server supports batched inputs: False
INFO 2024-10-21 02:01:23,450 instructlab.sdg.pipeline:197: Running block: gen_knowledge
INFO 2024-10-21 02:01:23,450 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3'],
    num_rows: 10
})
----

This will take several minutes to complete.

Once the process completes and we have generated additional data, we can use the `ilab model train` command to incorporate this dataset with the model.

If you are curious to view the data generated, the SDG process creates a jsonl file located in the `/home/instruct/.local/share/instructlab/datasets` directory named knowledge_train_msgs[TIMESTAMP].jsonl

TIP: JSONL files consist of multiple JSON objects, each on its own line.

Feel free to explore. You must input your exact file name in the following command:

[source,console]
----
cat /home/instruct/.local/share/instructlab/datasets/knowledge_train_msgs[YOUR_TIMESTAMP].jsonl
----

NOTE: Using a scale factor of 5 is generally not enough synthetic data to effectively impact the knowledge or skill of a model. However, due to time constraints of this workshop, the goal is to simply show you how this works using real commands. You would typically want to use a scale factor of 30 which is the default value to train the model effectively.

Once the new data has been generated, the next step is to train the model with the updated knowledge. This is performed with the `ilab model train` command.

[#changing_model]
== Enhancing a LLM with InstructLab

NOTE: Training using the newly generated data is a time and resource intensive task. Depending on the number of epochs desired, internet connection for safetensor downloading, and other factors, it can take many hours and is highly dependent on the hardware used.

To train the model, you will have to download the base model to start with. 

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository instructlab/granite-7b-lab
----

In order to train the model, quit the serving model with kbd:[Ctrl+C]. We can begin to train a model with your synthetically generated data. 

[source,console,role=execute,subs=attributes+]
----
ilab model train --data-path  /home/instruct/.local/share/instructlab/datasets/ --pipeline simple --device cuda --gpus 1
----

You would see the following output: 

[source,sh]
----
LINUX_TRAIN.PY: NUM EPOCHS IS:  1
LINUX_TRAIN.PY: TRAIN FILE IS:  taxonomy_data/train_gen.jsonl
LINUX_TRAIN.PY: TEST FILE IS:  taxonomy_data/test_gen.jsonl
LINUX_TRAIN.PY: Using device 'cuda:0'
  NVidia CUDA version: 12.1
  AMD ROCm HIP version: n/a
  cuda:0 is 'NVIDIA A10G' (15.3 GiB of 22.1 GiB free, capability: 8.6)
  WARNING: You have less than 18253611008 GiB of free GPU memory on '{index}'. Training may fail, use slow shared host memory, or move some layers to CPU.
  Training does not use the local InstructLab serve. Consider stopping the server to free up about 5 GiB of GPU memory.
LINUX_TRAIN.PY: LOADING DATASETS
Generating train split: 5 examples [00:00, 265.43 examples/s]
Generating train split: 7 examples [00:00, 6582.99 examples/s]
/home/instruct/instructlab/venv/lib64/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LINUX_TRAIN.PY: NOT USING 4-bit quantization
LINUX_TRAIN.PY: LOADING THE BASE MODEL
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  5.95it/s]
----

After the model is trained, it will be saved as a GGUF at `/home/instruct/.local/share/instructlab/checkpoints/ggml-model-f16.gguf`

CONGRATULATIONS! You just trained a chatbot for Parasol insurance and will make every claims agent’s life a little better!

=== Chatting with the new model

In order to serve the newly trained model you can now run the following in the *upper* command window:

[source,console,role=execute,subs=attributes+]
----
ilab model serve --model-path /home/instruct/.local/share/instructlab/checkpoints/ggml-model-f16.gguf
----

It may take some seconds to start, but you should see the following which should look familiar to you:

[source,console]
----
INFO 2024-10-20 17:24:33,497 instructlab.model.serve:136: Using model '/home/instruct/.local/share/instructlab/checkpoints/ggml-model-f16.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-10-20 17:24:33,497 instructlab.model.serve:140: Serving model '/home/instruct/.local/share/instructlab/checkpoints/ggml-model-f16.gguf' with llama-cpp
INFO 2024-10-20 17:24:34,492 instructlab.model.backends.llama_cpp:232: Replacing chat template:
 {% for message in messages %}
{% if message['role'] == 'user' %}
{{ '<|user|>
' + message['content'] }}
{% elif message['role'] == 'system' %}
{{ '<|system|>
' + message['content'] }}
{% elif message['role'] == 'assistant' %}
{{ '<|assistant|>
' + message['content'] + eos_token }}
{% endif %}
{% if loop.last and add_generation_prompt %}
{{ '<|assistant|>' }}
{% endif %}
{% endfor %}
INFO 2024-10-20 17:24:34,495 instructlab.model.backends.llama_cpp:189: Starting server process, press CTRL+C to shutdown server...
INFO 2024-10-20 17:24:34,495 instructlab.model.backends.llama_cpp:190: After application startup complete see http://127.0.0.1:8000/docs for API.
----

Because you’re serving the model in one terminal window, you will have to use a separate terminal window, run the ilab chat command and communicate with the model you are serving.
[source,console,role=execute,subs=attributes+]
----
ilab model chat -m /home/instruct/.local/share/instructlab/checkpoints/ggml-model-f16.gguf
----

[#verify]
=== Verifying the Application

Now for the moment of truth. You’ve added knowledge, generated synthetic data, and retrained the model. *Refresh your browser window* where you were viewing Marty McFly’s claim in the Parasol insurance application

You can kbd:[CTRL+C] to stop the previous chat but let the model serving run. 

image::parasol-view.png[]

Click on the blue chatbot icon in the bottom right corner of the screen to open the chatbot. If you already have it open you will need to start a new session by pressing the small kbd:[+] button on the bottom left-hand corner of the chat window.

image::parasol-chat.webp[width=350]

. Let’s ask the chatbot the same question with the newly trained model and see if the response has improved.

[source,console,role=execute,subs=attributes+]
----
How much does it cost to repair a flux capacitor?
----

You should see something similar to the following (keep in mind that your output may look different due to the nature of large language models):

image::parasol-new-response.webp[width=350]

[#conclusion]
== Conclusion

Woohoo young padawan, mission accomplished. Breathe in for a bit. We’re proud of you, and I dare say you’re an AI Engineer now. You’re probably wondering what the next steps are, so let me give you some suggestions.

Start playing with both skill and knowledge additions. This is to give something "new" to the model. You give it a chunk of data, something it doesn’t know about, and then train it on that. How could InstructLab-trained models help at your company? Which friend will you brag to first?

As you can see, InstructLab is pretty straightforward and most of the time you spend will be on curating new taxonomy content. Again, we’re so happy you made it this far, and remember if you have questions we are here to help, and are excited to see what you come up with!

Please visit the official project github at https://github.com/instructlab[www.github.com/instructlab] and check out the community repo to learn about how to get involved with the upstream community! Also, https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[learn more about RHEL AI here] (which includes support for InstructLab, idemification for model output for the included Granite large language models, and a platform to run AI your own way on the hybrid cloud).
